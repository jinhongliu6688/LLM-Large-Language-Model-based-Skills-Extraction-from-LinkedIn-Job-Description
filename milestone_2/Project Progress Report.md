# Project Progress Report
## Introduction:
In this project, we aim to extract skills from job postings using zero-shot or few-shot learning techniques, and use this data to train a large language model. We plan to build on the previous block's project by expanding the manually annotated corpus to create a larger labelled dataset. With a more comprehensive dataset, we hope to get a more accurate representation of the current job market and equip job seekers with the skills in demand for each role in tech. We will be utilizing the GPT-3 API for data labeling and prompt engineering to achieve optimal results. Our goal is to compare GPT-3's performance with human-annotated data and explore the capabilities and limitations of ChatGPT for data annotation.

## Motivations and Contributions/Originality
We are expanding upon the previous block's 523 project. With the recent layoffs in the tech industry, it has become important that candidates know what skills are in demand for each role in tech. Hence, we are working on labelling skills from job descriptions scraped from LinkedIn. We are expanding upon the manually annotated corpus from the previous block. With a bigger labelled corpus, we will get a more accurate representation of the current job market. We hope that users will be able to better equip themselves for their job search using our system.

## Data:

We are continuing with the job description data from the previous block's 523 project. This week we are trying to label this data by using GPT-3 APIs. We would try to achieve this by prompt engineering and few shot learning. Our plan is to compare GPT-3's performance with human annotated data. We have around 200 human annotated job descriptions and around 2000 unlabelled job descriptions.

## Engineering:

#### Computing Infrastructure:

Initially, we attempted to perform data labeling on personal computers using GPT-2 and T5 models. However, the results were not satisfactory. We decided to utilize the GPT-3 API to achieve better results. Therefore, we did not require any specific computing infrastructure such as Google Colab or Google Cloud TPUs.

#### DL-NLP Methods:

For data labeling, we utilized the text-curie-001 GPT-3 model. We fine-tuned the process by applying specific parameters such as the limit parameter for the token length of the paragraph and the temperature parameter for the randomness of the GPT-3 output.

#### Framework:

We created a Python script utilizing NumPy and Pandas libraries to implement the text-curie-001 GPT-3 model for data labeling. We did not use any existing codebase, as our approach involved utilizing the GPT-3 API. We fine-tuned the process by applying specific parameters to achieve optimal results



## Previous Works:

After looking at "Skill Extraction from Job Postings Using Pre-trained Language Models" by Zhang et al. we found a few more papers that seem relevant to our project. First, "Retrieving Skills from Job Descriptions: A Language Model Based Extreme Multi-label Classification Framework" (2020) by Bhola et al. This paper proposes a language model-based approach for extracting skills from job descriptions using extreme multi-label classification. The authors use several pre-trained language models and fine-tune them on a large dataset of job descriptions. They also conduct experiments to evaluate their method and compare it with several baseline methods. Overall, the paper presents a novel approach to skill extraction from job descriptions and provides valuable insights into the challenges of this task. 

Another paper we found interesting was "Occupational skills extraction with FinBERT" (2020) by Mariia Chernova. This paper proposes a method for extracting occupational skills from financial job postings using the FinBERT language model, which is pre-trained on financial text. The authors also conduct experiments to evaluate their method and compare it with several baseline methods. Overall, the paper presents a novel approach to skill extraction from job postings in a specific domain.


Lastly, we have "Salience and Market-aware Skill Extraction for Job Targeting" (2020) by Shi et al. The paper appears to make several contributions to the field of skill extraction and job targeting. The salience-aware attention mechanism is a novel approach to extracting relevant skills from job postings and is likely to be more effective than traditional keyword-based approaches. The market-aware weighting method is also an interesting idea, as it recognizes that the importance of certain skills may vary depending on the job market being targeted.
 
## Evaluation:

The annotations generated by the GPT-3 model are compared with the human annotations from the previous project. According to the [calculation](https://github.ubc.ca/MDS-CL-2022-23/COLX_585_GPT-5uperpowered_Sea_Urchins/blob/master/code/Evaluation.ipynb), the precision is around 26.31%, which indicates that the model makes some mistakes when identifying skills. The recall is around 11.42%. It suggests that the model misses some skills that are present in the data. The F1 score is around 15.92%, which is a combination of precision and recall and shows that the model's overall performance is not up to the mark for identifying skills. Therefore, there is a need for further improvements to make the GPT-3 model more dependable in identifying skills.

## Challenges

We faced a few challenges this week. Firstly, we had to decide on picking a model within OpenAI. We had to make trade-offs between how advanced the model is, and how expensive, time consuming it is. We decided to land on the text-curie-001 model. We also had to decide which prompt worked best. We experimented with several different prompts and landed on one that we decided was the best. We also had to make sure we get the data labelled in under a certain number of runs before we ran out of credits on the OpenAI site. Finally, since the skills extracted can be subjective at times, and not every would consider certain things as skills, we have had relatively lower scores.

After using GPT-3 to annotate skills for all the data, we found that it is difficult to match those data to the previous human annotated data, since the urls in the previous human annotated data are not unique. As a result, we decided to use GPT-3 to annotate skills from the job descriptions in the previous human annotated data separately and evaluate the result.

## Conclusion:

Our poor evaluation scores were likely a result of these challenges. The labeled data should still serve as a viable input for our project's use-case.


