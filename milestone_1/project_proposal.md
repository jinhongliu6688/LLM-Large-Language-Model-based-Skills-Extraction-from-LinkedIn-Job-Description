# Project proposal
## Introduction:
We have LinkedIn job postings data from our project in the previous block. We labelled some of this data to extract skills from job openings, we are going to try zero-shot or few shot learning to label the complete dataset. Then we plan on utilising this data further to train a large language model. The goal of this project is to explore the capabilities and limitations of ChatGPT for data annotation and develop a more efficient and effective way to train models using labelled data.

## Data:
We have more than 500 job postings scraped from LinkedIn, plus we have the code for scraping more data. We have labelled some of this data manually to extract skills required for the job from raw text.
Link to the data: https://github.ubc.ca/MDS-CL-2022-23/COLX_585_GPT-5uperpowered_Sea_Urchins/tree/Jaskirat/data

## Engineering:
#### Computing infrastructure:

We will start with Google Colab for initial setup and training, but we will switch to an AWS Deep Learning Instance if necessary for better performance. We may also consider using Google Cloud TPUs for faster training.

#### DL-NLP methods:

We plan to use deep learning models such as GPT-2 and BERT for question answering tasks. We may also consider using other models such as RoBERTa, ELECTRA, or T5 depending on the specific task requirements.

#### Framework:

We will use PyTorch as our primary deep learning framework for NLP. We can start off with existing codebases such as the Hugging Face Transformers library, which provides pre-trained models and a wide range of NLP task implementations that can be fine-tuned on our specific datasets. We will also refer to PyTorch's official documentation and tutorials to build our own custom models.

#### UI:

We plan to use HTML, CSS, and JavaScript to create an interactive web application that can showcase the capabilities of our trained models. We can use popular web frameworks such as Flask or FastAPI to handle the backend of the application and integrate it with our PyTorch models.

## Previous Works (minimal):
Previous works in this area have primarily focused on using LMs for natural language processing (NLP) tasks such as question answering, sentiment analysis, and text classification. However, there have been some recent studies that have explored the use of LMs for job skill extraction. One such study is "Skill Extraction from Job Postings Using Pre-trained Language Models" by Zhang et al. (2022), which proposes a method for extracting job skills using Weak Supervision.

## Evaluation:
The annotation of the skills generated by the GPT-2 model will be compared to the human annotations from the previous project. To access the performance, we will use various metrics, including accuracy, precision, recall, and F1 score. We might use some visualization to demonstrate our evaluation. It is important to note that human annotations from previous project may contain some errors due to biases.

## Conclusion:
This project aims to explore the capabilities and limitations of ChatGPT for data annotation and develop a more efficient and effective way to train models using labelled data to, ultimately, automatically extract key skills required for a job from LinkedIn job postings. The success of this project could have implications for the job market, as it could help job seekers better understand the skills required for a given job and tailor their resumes accordingly.
